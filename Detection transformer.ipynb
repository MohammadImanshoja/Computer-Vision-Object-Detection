{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86170e06-f700-49c4-9767-6cf97d6617da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO JSON file created successfully at C:\\Users\\moham\\Downloads\\Compressed\\tumrset\\Brain Tumor Detection\\annotations.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_coco(dataset_path, output_path):\n",
    "    # Initialize the COCO format structure\n",
    "    coco_format = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 0, \"name\": \"no_tumor\"},  # Added 'no_tumor' category\n",
    "            {\"id\": 1, \"name\": \"tumor_type_1\"},\n",
    "            {\"id\": 2, \"name\": \"tumor_type_2\"},\n",
    "            {\"id\": 3, \"name\": \"tumor_type_3\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    annotation_id = 1\n",
    "    image_id = 1\n",
    "\n",
    "    # Loop through each dataset subset\n",
    "    for subset in ['train', 'valid', 'test']:\n",
    "        images_folder = os.path.join(dataset_path, subset, 'images')\n",
    "        labels_folder = os.path.join(dataset_path, subset, 'labels')\n",
    "        \n",
    "        for image_filename in os.listdir(images_folder):\n",
    "            if not image_filename.endswith('.jpg'):\n",
    "                continue\n",
    "            \n",
    "            # Process image\n",
    "            image_path = os.path.join(images_folder, image_filename)\n",
    "            \n",
    "            # Check if the image file exists\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image {image_path} not found. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "            \n",
    "            image_entry = {\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": os.path.join(subset, 'images', image_filename),\n",
    "                \"width\": width,\n",
    "                \"height\": height\n",
    "            }\n",
    "            coco_format['images'].append(image_entry)\n",
    "            \n",
    "            # Process label\n",
    "            label_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
    "            label_path = os.path.join(labels_folder, label_filename)\n",
    "            \n",
    "            # If no corresponding label file, create a 'no_tumor' annotation\n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"Warning: Label file {label_filename} not found for image {image_filename}. Marking as 'no_tumor'.\")\n",
    "                \n",
    "                annotation_entry = {\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": 0,  # 'no_tumor' category\n",
    "                    \"bbox\": [],  # Empty bounding box\n",
    "                    \"area\": 0.0,  # No area\n",
    "                    \"iscrowd\": 0\n",
    "                }\n",
    "                coco_format['annotations'].append(annotation_entry)\n",
    "                annotation_id += 1\n",
    "                image_id += 1\n",
    "                continue\n",
    "            \n",
    "            with open(label_path, 'r') as label_file:\n",
    "                lines = label_file.readlines()\n",
    "                if not lines:\n",
    "                    # No objects in this image, mark as 'no_tumor'\n",
    "                    annotation_entry = {\n",
    "                        \"id\": annotation_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": 0,  # 'no_tumor' category\n",
    "                        \"bbox\": [],  # Empty bounding box\n",
    "                        \"area\": 0.0,  # No area\n",
    "                        \"iscrowd\": 0\n",
    "                    }\n",
    "                    coco_format['annotations'].append(annotation_entry)\n",
    "                    annotation_id += 1\n",
    "                else:\n",
    "                    for line in lines:\n",
    "                        elements = line.strip().split()\n",
    "                        if len(elements) != 5:\n",
    "                            print(f\"Warning: Incorrect label format in file {label_filename}. Skipping this line...\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Extract the YOLO format elements\n",
    "                        category_id, x_center, y_center, bbox_width, bbox_height = map(float, elements)\n",
    "                        \n",
    "                        # Convert from YOLO format to COCO format\n",
    "                        x_min = (x_center - bbox_width / 2) * width\n",
    "                        y_min = (y_center - bbox_height / 2) * height\n",
    "                        bbox_width = bbox_width * width\n",
    "                        bbox_height = bbox_height * height\n",
    "\n",
    "                        # COCO bbox format: [x_min, y_min, width, height]\n",
    "                        bbox = [x_min, y_min, bbox_width, bbox_height]\n",
    "                        \n",
    "                        annotation_entry = {\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(category_id) + 1,  # Shift by +1 to match category IDs\n",
    "                            \"bbox\": bbox,\n",
    "                            \"area\": bbox_width * bbox_height,\n",
    "                            \"iscrowd\": 0\n",
    "                        }\n",
    "                        coco_format['annotations'].append(annotation_entry)\n",
    "                        annotation_id += 1\n",
    "            \n",
    "            image_id += 1\n",
    "    \n",
    "    # Save the COCO format data to a JSON file\n",
    "    with open(output_path, 'w') as output_file:\n",
    "        json.dump(coco_format, output_file, indent=4)\n",
    "    print(f\"COCO JSON file created successfully at {output_path}\")\n",
    "\n",
    "# Specify the path to your dataset and the output path for the JSON file\n",
    "dataset_path = r'C:\\Users\\moham\\Downloads\\Compressed\\tumrset\\Brain Tumor Detection'\n",
    "output_json_path = os.path.join(dataset_path, 'annotations.json')\n",
    "\n",
    "# Generate the COCO JSON file\n",
    "convert_to_coco(dataset_path, output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108f8e36-26a3-4375-9049-bd3cfcda060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\moham\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\moham\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\moham\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\moham\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\moham\\anaconda3\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pytorch-lightning) (0.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\moham\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\moham\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (68.2.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\anaconda3\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio pytorch-lightning transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91efe2aa-c471-49ca-b116-9cdb8b614829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in c:\\users\\moham\\anaconda3\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pycocotools) (3.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\moham\\anaconda3\\lib\\site-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycocotools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb1ed5d-9c65-4f24-8bac-3a342dade128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\moham\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: torch in c:\\users\\moham\\anaconda3\\lib\\site-packages (from timm) (2.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\moham\\anaconda3\\lib\\site-packages (from timm) (0.18.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\moham\\anaconda3\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\moham\\anaconda3\\lib\\site-packages (from timm) (0.24.5)\n",
      "Requirement already satisfied: safetensors in c:\\users\\moham\\anaconda3\\lib\\site-packages (from timm) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\moham\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torch->timm) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->timm) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\moham\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->timm) (2021.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\moham\\anaconda3\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc5f2b-b57d-47c2-8b76-2545c3ef8086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([5, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                   | Params | Mode\n",
      "--------------------------------------------------------\n",
      "0 | model | DetrForObjectDetection | 41.5 M | eval\n",
      "--------------------------------------------------------\n",
      "41.3 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.010   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "399       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "C:\\Users\\moham\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae9d5d05c21422592b3619184565e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from transformers import DetrForObjectDetection\n",
    "\n",
    "# Step 1: Modify the BrainTumorDataset class to handle dataset splits\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, root, annotation_file, split='train', transforms=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.transforms = transforms\n",
    "        self.split = split\n",
    "        self.ids = self._filter_image_ids()\n",
    "\n",
    "    def _filter_image_ids(self):\n",
    "        valid_ids = []\n",
    "        for img_id in self.coco.imgs:\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            if self.split in img_info['file_name']:\n",
    "                ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "                if ann_ids:  # Include image ID only if annotations exist\n",
    "                    valid_ids.append(img_id)\n",
    "        return valid_ids\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.ids[index]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        path = image_info['file_name']\n",
    "        image = Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
    "        \n",
    "        annotations = self.coco.loadAnns(self.coco.getAnnIds(imgIds=image_id))\n",
    "        boxes = [ann['bbox'] for ann in annotations]\n",
    "        labels = [ann['category_id'] for ann in annotations]\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        \n",
    "        target = {\"boxes\": boxes, \"class_labels\": labels}\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "# Step 2: Modify the collate function to handle varying sizes of annotations\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]  # Extract images\n",
    "    labels = [item[1] for item in batch]  # Extract labels\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    batch_labels = []\n",
    "    for label in labels:\n",
    "        if label['boxes'].numel() > 0:\n",
    "            batch_labels.append({\n",
    "                'boxes': label['boxes'],\n",
    "                'class_labels': label['class_labels']\n",
    "            })\n",
    "        else:\n",
    "            batch_labels.append({\n",
    "                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "                'class_labels': torch.zeros((0,), dtype=torch.int64)\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': batch_labels\n",
    "    }\n",
    "\n",
    "# Step 3: Define the DetrLightningModule for training\n",
    "class DetrLightningModule(pl.LightningModule):\n",
    "    def __init__(self, lr, lr_backbone, weight_decay, num_labels):\n",
    "        super().__init__()\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"facebook/detr-resnet-50\", \n",
    "            num_labels=num_labels,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask=None):\n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"val_\" + k, v.item())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad], \"lr\": self.lr_backbone},\n",
    "        ]\n",
    "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "# Step 4: Load and prepare the datasets\n",
    "image_root = r'C:\\Users\\moham\\Downloads\\Compressed\\tumrset\\Brain Tumor Detection'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize datasets for training and validation\n",
    "train_dataset = BrainTumorDataset(root=image_root, annotation_file=os.path.join(image_root, 'annotations.json'), split='train', transforms=transform)\n",
    "val_dataset = BrainTumorDataset(root=image_root, annotation_file=os.path.join(image_root, 'annotations.json'), split='valid', transforms=transform)\n",
    "\n",
    "# Step 5: Create the DataLoaders with the updated collate function\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Step 6: Instantiate the model and the PyTorch Lightning trainer\n",
    "model = DetrLightningModule(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4, num_labels=4)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, devices=1, accelerator='cpu')\n",
    "\n",
    "# Pass both train and validation dataloaders to the trainer\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12622d9-ef34-4d45-98de-1c55cd3705f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Step 7: Load the test dataset\n",
    "test_dataset = BrainTumorDataset(root=image_root, annotation_file=annotation_file, transforms=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Step 8: Function to plot image with bounding boxes\n",
    "def plot_image_with_boxes(image, boxes, labels, color='r', title=None):\n",
    "    \"\"\" Plot image with bounding boxes in COCO format \"\"\"\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_min, y_min, width, height = box\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min, f'{label}', color='white', fontsize=12, verticalalignment='top', bbox={'facecolor': color, 'alpha': 0.5})\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Step 9: Iterate over the test dataloader and visualize predictions\n",
    "model.eval()  # Set model to evaluation mode\n",
    "for batch in test_dataloader:\n",
    "    images, targets = batch['pixel_values'], batch['labels']\n",
    "    images = images.to(model.device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    # Convert outputs to bounding boxes and labels\n",
    "    # Ensure that the predicted boxes are in the COCO format (x_min, y_min, width, height)\n",
    "    predicted_boxes = outputs.pred_boxes[0].detach().cpu().numpy()  # Assuming batch_size=1\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=-1)[0].detach().cpu().numpy()\n",
    "    \n",
    "    # Ground truth from the COCO-format JSON file\n",
    "    actual_boxes = targets[0]['boxes'].cpu().numpy()  # Assuming batch_size=1\n",
    "    actual_labels = targets[0]['class_labels'].cpu().numpy()\n",
    "    \n",
    "    # Plot the first image with actual and predicted bounding boxes\n",
    "    image_np = images[0].permute(1, 2, 0).cpu().numpy()  # Convert to numpy and channel-last format\n",
    "    plot_image_with_boxes(image_np, actual_boxes, actual_labels, color='g', title='Actual')\n",
    "    plot_image_with_boxes(image_np, predicted_boxes, predicted_labels, color='r', title='Predicted')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
